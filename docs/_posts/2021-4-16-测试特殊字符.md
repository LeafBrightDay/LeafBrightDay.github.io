---
layout: post
title: 常见的特殊字符
---
# 常见的特殊字符

**测试输入框常用特殊字符类别**

- 键盘上能输入的特殊字符
- 空格
- 货币符号：￥，$等
- 数学符号：=、不等于，求和等
- 非英文字母语言符号：a等汉语拼音
- 中文标点符号：，。、（）等
- 特殊汉字：囫囵等
- 转义序列：\n、\r、\t、\’等
- 系统保留字符：null、NULL等
- SQL语句：‘OR‘1’=’1等
- 脚本函数：`<‘script’>alter(“Test,Bom~~~”)<‘/script’>`
- html转义字符：&gt ;,&lt ;等
- 数值 包括0-9共十个数字，[0-9]
- 半角英文 包括26个英文字符的大小写，[A-Za-z]
- 半角英数 半角英文字母和数字，[A-Za-z0-9]
- 半角字符 所有可打印的半角字符，包括：~!@#$%^&*()_-+=|{}[]:;"’<>?,./和[A-Za-z0-9]
- 数据库用特殊字符 包括百分号(%)和单引号(‘)两个字符

# 特殊字符分类

1）特殊字符：空值，大部分的数据类型都是以NULL值表示，但有的数据库空和NULL是不同的

2）特殊字符：超过限制，如数字开头的表名。以上文举过的例子，可能dli本身支持数字开头的表名，但是中转格式avro不支持，也会导致迁移报错。

3）超出数据类型范围的值

对数值来说，超出范围后，正值读出来可能就是负值了，所以一般是要限制超范围值的写入，提前抛异常；对与字符类型的数据，超出后，可能被截断或者抛异常。

# 特殊字符导致的问题分类

## 哪里会存在特殊字符

需要客户输入的内容

1. 页面输入：名称、参数，例如集群名、作业名
2. API/SDK输入：配置、参数，如sql语句、提交命令中的参数
3. 用户的数据内容，如数据库的表数据；格式化保存的数据，如parquet、csv、json

## 为什么会出问题

1. 范围本身不对：类型或长度。

   例如ecs的命名只支持“中文、字母、数字和_、-、."，就需要保证范围内的组合都可用，范围外的组合都不可用。通常出现的问题是存在一些隐含限制没有写清楚，例如实际不能以数字开头，或不能以"-"开头，

   - 长度的问题一方面可能出现在边界值不对，另一方面可能出在与周边的对接，例如DLI的内表会自动创建obs桶，表名支持很长，但obs只能支持63字符
   - 格式自身限制，例如写parquet（版本1.8.3）文件，当写入的数据超过21亿行时，读的时候会报错；要通过升级parquet版本来解决

2. 对字符的处理过程中出了问题。例如某服务在权限处理的时候通过“."号来分隔字符，当作业名称中包含”."号时解析出来的字符串就不符合预期，导致权限校验失败。

3. 不满足业界习惯。例如提交spark作业，在参数中包含待“”号的json是常用场景，但是服务不支持。sql脚本中 – 被认为是注释符，服务没有特殊处理。

## 该如何测试

首先思考如下问题：

- 字段从哪里来：从console输入的字符，要考虑一些隐含的限制，如玲珑塔，会屏蔽一些sql语句。
- 字段会经过哪些处理：
  - 常见的功能类：转义、正则、sql相关的标准化。
  - 特殊处理：权限，例如细粒度权限是可以根据资源名来授权，那就要了解会影响权限处理的特殊字符。
  - 转义字符、空字符的处理等
- 字段到哪里去：如果字段会保存到数据库中，就需要考虑sql注入等问题；如果是提交给其他服务，则需要了解对应接口的限制，例如表名会同时在obs桶上创目录，那obs的目录名限制就成了表名的隐含限制。

总的来说，进行特殊字符的测试设计时，要考虑字符支持的范围、长度，了解实现细节，选取对应的特殊字符集，然后在根据功能影响范围，选择对应的功能场景进行组合。

# 典型问题

已隐去敏感信息

- 从console上提交spark-submit作业,当提交的执行程序参数为带""号的json体时，任务无法执行成功
- console上提交spark作业，但是–jars参数后面的框限制256个字符
- moggdb的数据同步，配置完源端和目标段的数据，点击下一步：页面左上角报：“网络异常”。在我们前端页面的展示：转义字符就没有了，嵌套双引号的html得转义字符才行，这个格式不能直接解析json格式
- 脚本导出，对脚本语句换行处理不友好，相关sql在编辑器无法识别换行，如下（已设置支持unix的\n模式） [图片] 看了下，导出来内容，转成了\n 字符，编辑器是当成正常字符
- 集群在扩容时指定节点hostname中出现’-'字符时会失败
- 脚本中注释字符“–”后的语句还会被执行，如果有中文，就会出错，需优化。
- oracle 迁移到 hive 的
  源表有字符串为NULL 迁移到 hive 非分区 储存格式为rcfile 时 是空字符串
- spark作业鉴权逻辑中无法处理作业名带点号的作业
- yarn页面查看日志中文乱码，但是container机器上的日志是正常的
